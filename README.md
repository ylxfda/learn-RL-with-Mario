# ğŸ§  Learn Reinforcement Learning with Mario

<p align="center">
  <img src="assets/wbx06.png" alt="Project hero illustration" width="800"/>
  <br>
  <em>Project hero illustration</em>
</p>

Welcome to **Learn Reinforcement Learning with Mario** â€” an educational repository that teaches you modern **reinforcement learning (RL)** through hands-on PyTorch implementations, using Super Mario as your guide.

**What you'll find here:**
- ğŸ“– **A brief RL introduction** â€” question-driven guide from REINFORCE to modern world models
- ğŸ¤– **Two RL algorithms:**
  - **PPO (Proximal Policy Optimization)** â€” learns from real experience
  - **DreamerV3** â€” learns by building a world model and "dreaming"

**Who is this for?**
Anyone curious about how AI agents learn to play games. No RL background required â€” we'll start from the basics and build up to state-of-the-art algorithms.

---

## ğŸ¯ Learning Philosophy

We believe that **curiosity is the best teacher**.

Perhaps you're here because you wondered: *"How do computers learn to play games?"* You've heard it's through something called **reinforcement learning**, but what exactly is that? And with so many RL algorithms out thereâ€”PPO and DreamerV3 were mentioned aboveâ€”what makes them different?

ğŸ‘‰ [Read: A Brief Introduction to Reinforcement Learning](A-Brief-Introduction-to-RL.md)

This question-driven guide walks you through the *why* behind each algorithm. You'll understand not just the formulas, but the **problems** each method solves and the **insights** that led to the next breakthrough. This guide is far from a comprehensive RL course, but it should be able to give you a quick overview of different RL algorithms and have you prepared to the next step.

Once you grasp the concepts, the next question probably is how theoretical ideas transform into working code? So far two RL algorithms have been implemented:
- **PPO** implementation in [`PPO/`](PPO/)
- **DreamerV3** implementation in [`dreamerv3/`](dreamerv3/)

As most variable and class naming follows the conventions from the original papers with detailed comments, the code is pretty much self-explanatory if you've read the papers.

---

## ğŸ§° Repository Structure

```
learn-rl-with-mario/
â”‚
â”œâ”€â”€ PPO/
â”‚   â”œâ”€â”€ ppo_agent.py          # PPO agent implementation
â”‚   â”œâ”€â”€ networks.py            # Actor and Critic networks
â”‚   â”œâ”€â”€ rollout_buffer.py     # Experience storage for PPO
â”‚   â””â”€â”€ README.md              # Detailed PPO documentation
â”‚
â”œâ”€â”€ dreamerv3/
â”‚   â”œâ”€â”€ world_model.py         # RSSM world model implementation
â”‚   â”œâ”€â”€ actor_critic.py        # Actor-Critic for DreamerV3
â”‚   â”œâ”€â”€ networks/              # Neural network components
â”‚   â”‚   â”œâ”€â”€ rssm.py           # Recurrent State Space Model
â”‚   â”‚   â””â”€â”€ encoder_decoder.py # CNN encoder/decoder
â”‚   â”œâ”€â”€ utils/                 # Helper utilities
â”‚   â”‚   â”œâ”€â”€ distributions.py  # Probability distributions
â”‚   â”‚   â””â”€â”€ tools.py          # Training utilities
â”‚   â””â”€â”€ README.md              # Detailed DreamerV3 documentation
â”‚
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ mario.py               # Mario environment wrapper
â”‚   â””â”€â”€ vec_mario.py           # Vectorized environments
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ ppo_config.yaml        # PPO hyperparameters
â”‚   â””â”€â”€ dreamer_config.yaml    # DreamerV3 hyperparameters
â”‚
â”œâ”€â”€ train_mario_ppo.py         # Training script for PPO
â”œâ”€â”€ train_mario_dreamer.py     # Training script for DreamerV3
â”œâ”€â”€ play_mario_ppo.py          # Visualize trained PPO agent
â”œâ”€â”€ play_mario_dreamer.py      # Visualize trained DreamerV3 agent
â”‚
â””â”€â”€ README.md                   # This file
```

---

## âš™ï¸ Getting Started



Once the enviroment is setup, you can start with either of the algorithms by reading the README file under the subdirectory.

Happy learning, and may your Mario reach the flag! ğŸš©

## ğŸ“œ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.