# ðŸ§  Learn Reinforcement Learning with Mario

Welcome to **Learn Reinforcement Learning with Mario** â€” an educational repository that teaches you the evolution of **policy gradient reinforcement learning (RL)** algorithms through the lens of the **Super Mario** game.

This repository contains **PyTorch implementations** of:
- âœ… **PPO (Proximal Policy Optimization)**
- âœ… **DreamerV3 (World Modelâ€“based RL)**

The goal of this README is to guide you from **zero RL background** to understanding **how and why** modern RL algorithms were designed â€” by answering a series of practical, question-driven learning steps.

---

## ðŸŽ® Why Mario?

Marioâ€™s environment is the perfect playground for understanding RL concepts:
- He sees the world (state).
- He chooses actions (move, jump, run).
- He receives feedback (reward).
- He learns to **maximize long-term success**.

---

## ðŸ§© What Is Reinforcement Learning?

Reinforcement Learning (RL) teaches an agent how to act by interacting with an environment.

At each time step:
- The agent observes a **state** \( s_t \),
- Takes an **action** \( a_t \),
- Receives a **reward** \( r_t \),
- And transitions to the next state \( s_{t+1} \).

The goal is to learn a **policy** \( \pi_\theta(a|s) \) â€” a mapping from states to actions â€” that maximizes the **expected cumulative reward**:

\[
J(\theta) = \mathbb{E}_{\pi_\theta} \Big[ \sum_{t=0}^\infty \gamma^t r_t \Big]
\]

---

## ðŸªœ Step-by-Step Evolution of RL Algorithms (with Mario Examples)

Below we explore the major milestones â€” from **REINFORCE** to **PPO**, and finally to **DreamerV3** â€” always asking:

> ðŸ’¡ What problem are we solving at each step?

---

### 1ï¸âƒ£ REINFORCE â€” *â€œCan Mario learn just from rewards?â€*

**Idea:**  
After each episode, update the policy based on total reward.

**Update rule:**
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a_t|s_t) R_t] \tag{1}
\]

**Intuition:**  
If a sequence of actions leads to a high reward, make those actions more likely.

**Mario Example:**  
Mario randomly jumps; if he survives longer, reinforce those jumps.

**Problem:**  
- Very high variance â€” results change wildly from one episode to another.  
- Learns slowly â€” feedback only comes at the end.

---

### 2ï¸âƒ£ Add a Baseline â€” *â€œCan Mario judge actions relative to his usual performance?â€*

**Idea:**  
Subtract a baseline value \( V^\pi(s_t) \) representing expected performance to reduce noise.

**Update rule:**
\[
\nabla_\theta J(\theta) = \mathbb{E}[\nabla_\theta \log \pi_\theta(a_t|s_t)(R_t - V^\pi(s_t))] \tag{2}
\]

**Advantage function:**
\[
A_t = R_t - V^\pi(s_t)
\]

**Mario Example:**  
If Mario usually earns +5 coins but now earns +10, he learns that this jump was better than usual.

**Benefit:**  
Reduces variance â†’ more stable learning.

---

### 3ï¸âƒ£ Actorâ€“Critic â€” *â€œCan Mario get feedback immediately instead of waiting until he dies?â€*

**Idea:**  
Add a **Critic** network to estimate \( V(s_t) \) (the baseline) while the **Actor** updates the policy.

**Temporal-Difference Advantage:**
\[
A_t = r_t + \gamma V(s_{t+1}) - V(s_t) \tag{3}
\]

**Mario Example:**  
Now Mario gets real-time feedback â€” every frame tells him whether heâ€™s improving or not.

**Benefit:**  
- Online updates (no need for full episodes).  
- Faster, more continuous learning.

---

### 4ï¸âƒ£ A2C / A3C â€” *â€œCan many Marios learn in parallel?â€*

**Idea:**  
Run multiple Mario agents simultaneously in parallel environments.  
Each agent collects experiences and contributes gradients to a shared model.

**Benefit:**  
- Faster data collection.  
- Smoother gradient estimation.  
- More stable learning.

---

### 5ï¸âƒ£ TRPO â€” *â€œHow can Mario avoid sudden, catastrophic policy changes?â€*

**Problem:**  
Even with a critic, large updates can cause the policy to change too drastically.

**Solution:**  
Add a **trust region** constraint â€” restrict how much the new policy can deviate from the old one.

\[
\begin{aligned}
\max_\theta &\ \mathbb{E}_t\left[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}A_t\right] \\
\text{s.t. } &\ \mathbb{E}_t[KL(\pi_{\theta_{\text{old}}} \| \pi_\theta)] \le \delta
\end{aligned} \tag{4}
\]

**Mario Example:**  
Mario doesnâ€™t completely change his jumping style overnight; he takes safe, measured steps in learning.

**Drawback:**  
Computationally expensive due to second-order gradient constraints.

---

### 6ï¸âƒ£ PPO â€” *â€œCan we simplify safe updates while keeping them stable?â€*

**Idea:**  
Replace the hard constraint of TRPO with an easy-to-compute **clipped surrogate objective**.

\[
L^{CLIP}(\theta) = \mathbb{E}_t \Big[
\min\big(
r_t(\theta)A_t,\ \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t
\big)
\Big] \tag{5}
\]

where  
\[
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}
\]

**Mario Example:**  
If Marioâ€™s new policy changes too much (say, more than Â±20%), we clip it to keep updates stable.

**Benefits:**  
- Simple implementation.  
- High performance.  
- Stable learning in complex environments.

This is the first algorithm included in this repo.

---

### 7ï¸âƒ£ DreamerV3 â€” *â€œCan Mario imagine before acting?â€*

**Problem with PPO:**  
It learns only from **real interactions**, requiring millions of frames.  
Mario must die many times to improve.

**Idea:**  
Teach Mario to **build a world model** â€” an internal simulation of how the game behaves â€” and learn by â€œdreamingâ€ inside it.

---

#### ðŸ§  Core Components

1. **World Model (Encoder + Transition + Decoder):**  
   Learns to compress observations into latent states \( z_t \) and predict next states, rewards, and continuation signals.

2. **Imagination Rollouts:**  
   Generates imaginary trajectories \( (z_t, a_t, r_t) \) within the latent space instead of the real game.

3. **Actor & Critic in Latent Space:**  
   Uses imagined trajectories to train the policy and value functions efficiently.

---

**Training Loop Overview:**

1. Collect real experiences for a short time.  
2. Train the world model to predict future states.  
3. Use the model to â€œimagineâ€ many future rollouts.  
4. Optimize policy and value inside the imagined world.  
5. Occasionally update with real experiences.

---

**Mario Example:**  
Mario watches a few rounds of gameplay, learns how the world behaves, and then mentally simulates thousands of jumps, enemy encounters, and coin collections â€” all in his mind â€” before trying them in the real game.

**Benefits:**
- Learns from far fewer real frames.  
- Much faster and safer training.  
- Generalizes better.

This is the **second algorithm** implemented in this repo.

---

## ðŸ” Summary: Evolution of Marioâ€™s Learning

| Stage | Algorithm | Key Idea | Marioâ€™s Learning Style |
|--------|------------|-----------|------------------------|
| 1ï¸âƒ£ | REINFORCE | Learn from total reward | Trial and error |
| 2ï¸âƒ£ | + Baseline | Compare to average | Learns relative success |
| 3ï¸âƒ£ | Actorâ€“Critic | Add a value estimator | Real-time feedback |
| 4ï¸âƒ£ | A2C/A3C | Parallel agents | Multiple worlds |
| 5ï¸âƒ£ | TRPO | Limit policy change | Careful improvement |
| 6ï¸âƒ£ | PPO | Simplify safe updates | Balanced, efficient learning |
| 7ï¸âƒ£ | DreamerV3 | Learn a world model | Imagines and plans ahead |

---

## ðŸ§° Repository Structure

```
Learn-Reinforcement-Learning-with-Mario/
â”‚
â”œâ”€â”€ PPO/
â”‚   â”œâ”€â”€ ppo_agent.py
â”‚   â”œâ”€â”€ ppo_train.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ DreamerV3/
â”‚   â”œâ”€â”€ dreamer_agent.py
â”‚   â”œâ”€â”€ dreamer_train.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ envs.py      # Mario Gym environment wrappers
â”‚   â””â”€â”€ plotting.py  # Visualization helpers
â”‚
â””â”€â”€ README.md         # (this file)
```

---

## âš™ï¸ Getting Started

### Installation
```bash
git clone https://github.com/yourname/Learn-Reinforcement-Learning-with-Mario.git
cd Learn-Reinforcement-Learning-with-Mario
pip install -r requirements.txt
```

### Run PPO Training
```bash
python PPO/ppo_train.py
```

### Run DreamerV3 Training
```bash
python DreamerV3/dreamer_train.py
```

---

## ðŸ“˜ References

- Williams, R. J. (1992). *Simple statistical gradient-following algorithms for connectionist reinforcement learning.*
- Schulman et al. (2015). *Trust Region Policy Optimization.*
- Schulman et al. (2017). *Proximal Policy Optimization Algorithms.*
- Hafner et al. (2023). *Mastering Diverse Domains through World Models (DreamerV3).*

---

### ðŸ’¬ Final Thought

> PPO taught Mario to **learn steadily from real experiences.**  
> DreamerV3 taught Mario to **think and plan inside his own imagination.**

---

**Enjoy exploring, modifying, and training your own Mario agent!**
