<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Brief Introduction to Reinforcement Learning</title>
    
    <!-- MathJax Configuration -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                tags: 'ams'
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
    
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica', 'Arial', sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            background-color: #fff;
        }
        
        h1 {
            border-bottom: 2px solid #eaecef;
            padding-bottom: 0.3em;
            font-size: 2em;
            margin-top: 24px;
            margin-bottom: 16px;
        }
        
        h2 {
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
            font-size: 1.5em;
            margin-top: 24px;
            margin-bottom: 16px;
        }
        
        h3 {
            font-size: 1.25em;
            margin-top: 24px;
            margin-bottom: 16px;
        }
        
        h4 {
            font-size: 1em;
            margin-top: 24px;
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 85%;
        }
        
        pre {
            background-color: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            line-height: 1.45;
        }
        
        pre code {
            background-color: transparent;
            padding: 0;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            padding-left: 16px;
            color: #6a737d;
            margin-left: 0;
        }
        
        hr {
            height: 0.25em;
            padding: 0;
            margin: 24px 0;
            background-color: #e1e4e8;
            border: 0;
        }
        
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 16px 0;
        }
        
        table th, table td {
            border: 1px solid #dfe2e5;
            padding: 6px 13px;
        }
        
        table th {
            background-color: #f6f8fa;
            font-weight: 600;
        }
        
        table tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        
        ul, ol {
            padding-left: 2em;
        }
        
        li {
            margin: 0.25em 0;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        .mjx-chtml {
            font-size: 108% !important;
        }
        
        /* Ensure equation numbers appear on the right */
        mjx-container[display="true"][width="full"] {
            display: flex !important;
            justify-content: space-between !important;
        }
    </style>
</head>
<body>

<h1>ü™ú A Brief Introduction to Reinforcement Learning</h1>

<p>
This repository's main goal is to help you learn and implement PPO and DreamerV3 algorithms. However, when reading original papers for these two algorithms, the most common question is: <strong>"Why is it designed this way?"</strong>
</p>

<p>
The best way to answer this question is to understand how RL evolved from the simplest REINFORCE method to PPO, and eventually to the Dreamer series of world model-based algorithms. This tutorial serves exactly that purpose ‚Äî it provides a self-contained introduction tracing this evolution step by step, building up the key concepts and motivations behind each algorithmic innovation. No prior RL knowledge required!
</p>

<hr>

<h2>üìã Table of Contents</h2>

<ul>
<li><a href="#-what-is-reinforcement-learning">What Is Reinforcement Learning?</a></li>
<li><a href="#the-evolution-of-rl-algorithms-a-question-driven-journey">The Evolution of RL Algorithms</a></li>
<p>
  - <a href="#1Ô∏è‚É£-reinforce-can-we-learn-directly-from-rewards">1Ô∏è‚É£ REINFORCE</a>
  - <a href="#2Ô∏è‚É£-baseline-how-do-we-reduce-the-noise">2Ô∏è‚É£ Baseline</a>
  - <a href="#3Ô∏è‚É£-actor-critic-can-we-learn-online-step-by-step">3Ô∏è‚É£ Actor-Critic</a>
  - <a href="#4Ô∏è‚É£-a2ca3c-what-if-many-marios-learn-simultaneously">4Ô∏è‚É£ A2C/A3C</a>
  - <a href="#5Ô∏è‚É£-trpo-how-do-we-ensure-safe-gradual-improvement">5Ô∏è‚É£ TRPO</a>
  - <a href="#6Ô∏è‚É£-ppo-can-we-simplify-safe-updates">6Ô∏è‚É£ PPO</a>
  - <a href="#7Ô∏è‚É£-dreamerv3-can-we-learn-by-dreaming">7Ô∏è‚É£ DreamerV3</a>
<li><a href="#-summary-the-journey-from-reinforce-to-dreamerv3">Summary Table</a></li>
<li><a href="#-references-and-further-reading">References</a></li>
</p>
</ul>

<hr>

<h2>üß© What Is Reinforcement Learning?</h2>

<strong>The Core Idea:</strong>
<p>
Reinforcement learning teaches an agent (like Mario) to make decisions through trial and error, guided by rewards. Think of it as learning to play Super Mario Bros: Mario doesn't start knowing how to beat the level ‚Äî he learns by trying different actions and getting feedback.
</p>

<strong>The RL Loop (with Mario as example):</strong>
<p>
At each time step $t$:
</p>

<p>
1. <strong>Agent observes state</strong> $s_t$
   - <em>Mario sees</em>: enemies, blocks, pipes, gaps, his position on screen
</p>

<p>
2. <strong>Agent selects action</strong> $a_t$ based on its policy $\pi_\theta(a|s)$
   - <em>Mario chooses</em>: move right, jump, run+jump, or do nothing
</p>

<p>
3. <strong>Environment returns reward</strong> $r_t$ and next <strong>state</strong> $s_{t+1}$
   - <em>Mario receives</em>: +1 for moving forward, +100 for collecting coins, +1000 for reaching the flag, -15 for dying
</p>

<p>
4. <strong>Agent updates its policy</strong> to get better rewards in the future
   - <em>Mario learns</em>: "Jumping at that gap was good!" or "Walking into that Goomba was bad!"
</p>

<strong>The Objective:</strong>
<p>
Learn a policy $\pi_\theta$ that maximizes <strong>expected cumulative reward</strong>:
</p>

<p>
$$
J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right] \tag{1}
$$
</p>

<p>
where $\gamma \in [0,1)$ is the discount factor.
</p>

<strong>Why "Expected" Reward?</strong>
<p>
We maximize the <strong>expectation</strong> (average over many runs) rather than a single episode's reward because:
</p>
<ul>
<li><strong>Stochastic environments</strong>: Mario might encounter different enemy patterns or random spawns</li>
<li><strong>Stochastic policies</strong>: Mario samples actions probabilistically (e.g., 70% jump, 30% run)</li>
<li><strong>Generalization</strong>: We want a policy that consistently reaches the flag, not one that got lucky once</li>
</ul>

<strong>Why Discount Factor $\gamma$?</strong>
<p>
The discount factor serves three purposes:
</p>

<p>
1. <strong>Mathematical convergence</strong>: Without discounting ($\gamma=1$), the infinite sum might diverge (become infinite)
2. <strong>Preference for sooner rewards</strong>: Getting +100 points now is better than getting +100 points in 1000 steps ‚Äî this encourages efficient solutions
3. <strong>Uncertainty about distant rewards</strong>: In Mario, episodes end when he dies or reaches the flag, so distant rewards are less certain
   - With $\gamma=0.99$, a reward 100 steps away is worth only $0.99^{100} \approx 0.37$ of its nominal value
   - This encourages Mario to reach the flag quickly rather than wandering aimlessly
</p>

<hr>

<h2>The Evolution of RL Algorithms: A Question-Driven Journey</h2>

<p>
Let's trace the path from simple policy gradient methods to modern world models. At each stage, we'll ask: <strong>"What's wrong with what we have, and how can we fix it?"</strong>
</p>

<hr>

<h3>1Ô∏è‚É£ REINFORCE: <em>Can we learn directly from rewards?</em></h3>

<strong>The Starting Point:</strong>
<p>
What if Mario just tries random actions, and after each episode, we make actions that led to good outcomes more likely?
</p>

<strong>How it works:</strong>
<p>
After completing an episode, compute the total return $G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k$ from each time step, then update:
</p>

<p>
$$
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a_t|s_t) \, G_t] \tag{1}
$$
</p>

<strong>Step-by-Step Derivation:</strong>

<p>
Let's derive this equation from first principles. Our goal is to maximize $J(\theta) = \mathbb{E}_{\pi_\theta}[G_0]$, where $G_0$ is the total return from the start of an episode.
</p>

<strong>Step 1: Expand the expectation</strong>

<p>
The expectation is taken over trajectories $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots)$ generated by policy $\pi_\theta$:
</p>

<p>
$$J(\theta) = \sum_{\tau} P(\tau | \theta) \, G(\tau)$$
</p>

<p>
where $P(\tau | \theta)$ is the probability of trajectory $\tau$ under policy $\pi_\theta$, and $G(\tau)$ is the return of that trajectory.
</p>

<strong>Step 2: Compute the gradient</strong>

<p>
Take the gradient with respect to $\theta$:
</p>

<p>
$$\nabla_\theta J(\theta) = \sum_{\tau} \nabla_\theta P(\tau | \theta) \, G(\tau)$$
</p>

<strong>Step 3: Apply the log-derivative trick</strong>

<p>
Use the identity $\nabla_\theta P(\tau | \theta) = P(\tau | \theta) \, \nabla_\theta \log P(\tau | \theta)$:
</p>

<p>
$$\nabla_\theta J(\theta) = \sum_{\tau} P(\tau | \theta) \, \nabla_\theta \log P(\tau | \theta) \, G(\tau)$$
</p>

<p>
This can be rewritten as an expectation:
</p>

<p>
$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [\nabla_\theta \log P(\tau | \theta) \, G(\tau)]$$
</p>

<strong>Step 4: Expand the trajectory probability</strong>

<p>
The probability of a trajectory is:
</p>

<p>
$$P(\tau | \theta) = p(s_0) \prod_{t=0}^{T-1} \pi_\theta(a_t|s_t) \, p(s_{t+1}|s_t, a_t)$$
</p>

<p>
Taking the log:
</p>

<p>
$$\log P(\tau | \theta) = \log p(s_0) + \sum_{t=0}^{T-1} \left[\log \pi_\theta(a_t|s_t) + \log p(s_{t+1}|s_t, a_t)\right]$$
</p>

<strong>Step 5: Compute gradient of log probability</strong>

<p>
When we take $\nabla_\theta$, the initial state distribution $p(s_0)$ and transition dynamics $p(s_{t+1}|s_t, a_t)$ don't depend on $\theta$, so they vanish:
</p>

<p>
$$\nabla_\theta \log P(\tau | \theta) = \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t)$$
</p>

<strong>Step 6: Substitute back</strong>

<p>
Plugging this into our gradient expression:
</p>

<p>
$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \, G(\tau)\right]$$
</p>

<strong>Step 7: Refine to per-timestep form</strong>

<p>
Since the return from timestep $t$ is $G_t = \sum_{k=t}^{T-1} \gamma^{k-t} r_k$, and rewards after time $t$ don't depend on actions before $t$, we can write:
</p>

<p>
$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \, G_t\right]$$
</p>

<p>
In practice, we sample this expectation by running episodes, giving us the REINFORCE update rule.
</p>

<strong>Key Insights:</strong>
<ul>
<li>The gradient pushes the policy to increase the probability of actions that led to high returns</li>
<li>The environment dynamics disappear from the gradient (we don't need to know how the environment works!)</li>
<li>We can estimate this gradient by sampling trajectories from $\pi_\theta$</li>
</ul>

<strong>What does "sampling trajectories" mean in practice?</strong>

<p>
The gradient formula contains an <strong>expectation</strong> $\mathbb{E}_{\pi_\theta}[\cdot]$, which theoretically requires averaging over <em>all possible trajectories</em> that could be generated by policy $\pi_\theta$. However:
</p>

<p>
1. <strong>Problem</strong>: There are infinitely many (or exponentially many) possible trajectories
   - Mario could take infinitely many different sequences of actions
   - We can't enumerate and compute the gradient for each one
</p>

<p>
2. <strong>Solution (Monte Carlo Estimation)</strong>: Instead of computing the exact expectation, we <strong>approximate</strong> it by:
   - Running the policy $\pi_\theta$ in the environment (e.g., playing Mario) to generate a few sample trajectories
   - Computing the gradient for these sampled trajectories
   - Using their average as an estimate of the true expectation
</p>

<p>
3. <strong>Practical Algorithm</strong>:
   ``<code>python
   # Run N episodes with current policy œÄ_Œ∏
   for episode in range(N):
       trajectory = run_episode(policy=œÄ_Œ∏)  # Sample one trajectory
       G_t = compute_returns(trajectory)
</p>

<p>
       # Compute gradient for this trajectory
       for t in range(len(trajectory)):
           gradient += ‚àá_Œ∏ log œÄ_Œ∏(a_t|s_t) * G_t
</p>

<p>
   # Average the gradients (estimate the expectation)
   gradient = gradient / N
</p>

<p>
   # Update parameters
   Œ∏ ‚Üê Œ∏ + learning_rate * gradient
   </code>`<code>
</p>

<p>
4. <strong>Why this works (Law of Large Numbers)</strong>:
   - As we collect more trajectories, the sample average converges to the true expectation
   - Even with just a few trajectories (e.g., 10-20 episodes), we get a reasonable estimate
</p>

<strong>Mario Example:</strong>
<p>
Instead of imagining every possible way Mario could play the game, we just have Mario play 10 times. If he reaches the flag in 8 out of 10 games by mostly pressing "right" and "jump", we increase the probability of those actions. We don't need to know what would have happened in the billions of other ways he could have played!
</p>

<strong>Intuition:</strong>
<p>
If a sequence of actions led to high reward, increase their probability. If it led to low reward, decrease it.
</p>

<strong>Mario's Experience:</strong>
<p>
Mario tries jumping randomly. If he survives longer in one episode, he reinforces those specific jumps.
</p>

<strong>‚ùå The Problem:</strong>

<strong>1. High Variance</strong> - Results swing wildly between episodes

<em>Why is high variance bad?</em>

<p>
Even though our gradient estimate is <strong>unbiased</strong> (correct on average), high variance means individual estimates are unreliable:
</p>

<ul>
<li><strong>Noisy gradients</strong>: One episode might give gradient pointing left, next episode points right</li>
<li><strong>Requires small learning rates</strong>: To avoid instability from bad gradient estimates, we must take tiny steps ‚Üí slow learning</li>
<li><strong>Needs many samples</strong>: To average out the noise, we need many episodes per update ‚Üí sample inefficient</li>
<li><strong>Unstable training</strong>: Performance bounces around instead of steadily improving</li>
</ul>

<strong>Mario Example of High Variance:</strong>
</code>`<code>
<p>
Episode 1: Mario dies immediately ‚Üí G_0 = -15 ‚Üí "pressing right is bad!"
Episode 2: Mario reaches flag ‚Üí G_0 = 3000 ‚Üí "pressing right is great!"
Episode 3: Mario falls in pit ‚Üí G_0 = 100 ‚Üí "pressing right is mediocre..."
Episode 4: Mario gets stuck ‚Üí G_0 = 50 ‚Üí "pressing right is terrible!"
</code>``
</p>

<p>
The gradient estimate keeps changing drastically! The algorithm doesn't know which signal to trust. If we use a large learning rate, the policy will swing wildly between these conflicting signals and never converge.
</p>

<strong>Mathematically</strong>: Variance affects convergence speed

<p>
If gradient estimates have variance $\sigma^2$, then to get an estimate within $\epsilon$ of the true gradient with high probability, we need roughly $O(\sigma^2/\epsilon^2)$ samples. Higher variance ‚Üí need exponentially more data!
</p>

<strong>2. Slow Learning</strong> - Feedback only comes at the end of entire episodes

<strong>3. Credit Assignment</strong> - Hard to know <em>which</em> actions were actually good

<p>
You might wonder: "But Mario gives us rewards at <em>every</em> timestep (e.g., +1 for moving right, +100 for coins, +1000 for flag). Why is credit assignment still a problem?"
</p>

<strong>The Key Issue</strong>: Even though the environment provides individual rewards $r_t$ at each timestep, <strong>REINFORCE uses the cumulative return</strong> $G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots$ to weight the gradient. This aggregation loses information about <em>which specific actions</em> were responsible for the outcome.

<strong>Mario Example:</strong>

<p>
Consider a successful episode that reaches the flag with total return $G_0 = 3000$:
</p>

<p>
``<code>
t=0:   Jump over first goomba      ‚Üí Critical action! (would die otherwise)
t=10:  Press RIGHT in empty hallway ‚Üí Trivial action (just moving forward)
t=50:  Jump to avoid pit            ‚Üí Critical action! (would fall otherwise)
t=200: Jump to grab flag            ‚Üí Critical action! (completes level)
</code>``
</p>

<p>
REINFORCE updates ALL these actions with the same weight:
</p>

<p>
$$
\nabla_\theta \log \pi_\theta(a_0|s_0) \cdot 3000 \quad \text{(jump over goomba)}
$$
$$
\nabla_\theta \log \pi_\theta(a_{10}|s_{10}) \cdot 3000 \quad \text{(press right)}
$$
$$
\nabla_\theta \log \pi_\theta(a_{50}|s_{50}) \cdot 3000 \quad \text{(jump over pit)}
$$
$$
\nabla_\theta \log \pi_\theta(a_{200}|s_{200}) \cdot 3000 \quad \text{(grab flag)}
$$
</p>

<strong>The Problem:</strong>
<ul>
<li>The critical actions (jumping over goomba, avoiding pit, grabbing flag) and the trivial action (moving right in empty space) all get the <strong>same credit</strong> (3000)</li>
<li>We cannot distinguish which actions were <em>actually</em> responsible for success</li>
<li>An episode with 300 actions means 300 actions all get credited equally, even though maybe only 10 were crucial</li>
</ul>

<strong>Why This Matters:</strong>
<ul>
<li>Wastes learning signal on irrelevant actions</li>
<li>Slow to identify truly important decisions</li>
<li>May reinforce lucky but suboptimal behaviors</li>
</ul>

<strong>What we'd prefer:</strong> Some way to assign <em>more</em> credit to actions that had bigger impact (like the jumps over dangers) and <em>less</em> credit to routine actions (like moving right in safe areas). This is what other more advanced RL algorithms try to address, which we'll learn next!

<blockquote>ü§î <strong>Next Question:</strong> Can we make learning more stable by judging actions relative to "typical" performance?</blockquote>

<hr>

<h3>2Ô∏è‚É£ Baseline: <em>How do we reduce the noise?</em></h3>

<strong>The Insight:</strong>
<p>
Absolute rewards don't matter ‚Äî what matters is whether we did <em>better or worse than usual</em>.
</p>

<strong>The Fix:</strong>
<p>
Subtract a baseline $b(s_t)$ (typically the value function $V^\pi(s_t)$) from the return:
</p>

<p>
$$
\nabla_\theta J(\theta) = \mathbb{E}[\nabla_\theta \log \pi_\theta(a_t|s_t) \, (G_t - b(s_t))] \tag{2}
$$
</p>

<strong>What is the Value Function?</strong>

<p>
The <strong>value function</strong> $V^\pi(s)$ estimates the expected cumulative return starting from state $s$ and following policy $\pi$:
</p>

<p>
$$
V^\pi(s) = \mathbb{E}_{\pi} \left[ G_t \mid s_t = s \right] = \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k} \mid s_t = s \right]
$$
</p>

<strong>Intuition</strong>: $V^\pi(s)$ answers the question: "If Mario is in state $s$ (e.g., standing before a goomba), how much total reward can he expect to get from now until the end, following his current policy?"

<strong>How is the Value Function Calculated?</strong>

<p>
In the baseline method, we typically estimate $V^\pi(s)$ using <strong>Monte Carlo estimation</strong>:
</p>

<p>
1. <strong>Collect episodes</strong>: Run policy $\pi$ multiple times and observe returns from each state
2. <strong>Average returns</strong>: For each state $s$, average all the returns $G_t$ observed from that state
3. <strong>Result</strong>: $V^\pi(s) \approx \frac{1}{N} \sum_{i=1}^{N} G_t^{(i)}$ where we visited state $s$ $N$ times
</p>

<strong>Mario Example:</strong>
<ul>
<li>Every time Mario is at "position 100 with goomba ahead", we record what happened next</li>
<li>Episode 1: Mario jumps, gets +2000 total</li>
<li>Episode 2: Mario jumps, gets +1800 total</li>
<li>Episode 3: Mario walks into it, gets -15 total</li>
<li>Average: $V^\pi(\text{goomba ahead}) \approx \frac{2000 + 1800 + (-15)}{3} \approx 1262$</li>
</ul>

<p>
In practice, $V^\pi$ is often approximated using a <strong>neural network</strong> (called the critic) that learns to predict these expected returns, which we'll see in the Actor-Critic method next.
</p>

<strong>Advantage Function:</strong>
<p>
Define $A_t = G_t - V^\pi(s_t)$ as the <strong>advantage</strong> ‚Äî how much better this action was compared to average.
</p>

<strong>Mario's Experience:</strong>
<p>
If Mario usually scores 500 points but this time scores 800, he knows this run was particularly good. He focuses on <em>what he did differently</em>, not the absolute score.
</p>

<strong>‚úÖ The Improvement:</strong>
<p>
Much lower variance ‚Üí more stable learning.
</p>

<strong>‚ùå Still a Problem:</strong>
<p>
Mario still has to wait until the episode ends to learn anything.
</p>

<blockquote>ü§î <strong>Next Question:</strong> Can we get feedback <em>during</em> the episode instead of waiting until Mario dies?</blockquote>

<hr>

<h3>3Ô∏è‚É£ Actor-Critic: <em>Can we learn online, step-by-step?</em></h3>

<strong>The Insight:</strong>
<p>
Instead of waiting for the full episode return $G_t$, estimate it using a <strong>critic</strong> network.
</p>

<strong>The Architecture:</strong>
<ul>
<li><strong>Actor</strong> $\pi_\theta(a|s)$: Chooses actions</li>
<li><strong>Critic</strong> $V_\phi(s)$: Estimates how good states are</li>
</ul>

<strong>Temporal-Difference (TD) Advantage:</strong>
<p>
Replace $G_t$ with a one-step estimate:
</p>

<p>
$$
A_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t) \tag{3}
$$
</p>

<p>
This is called the <strong>TD error</strong> ‚Äî it measures whether the actual reward plus next state's value exceeded our prediction.
</p>

<strong>Mario's Experience:</strong>
<p>
Every single frame, Mario gets immediate feedback: "Was this jump good or bad?" He doesn't need to die first.
</p>

<strong>‚úÖ The Improvement:</strong>
<ul>
<li>Online learning (no need to finish episodes)</li>
<li>Faster, more responsive updates</li>
<li>Better credit assignment</li>
</ul>

<strong>‚ùå Still a Problem:</strong>

<strong>1. Sample Inefficiency</strong>: Each frame is used once for updates, then discarded. Unlike methods that use replay buffers, Actor-Critic doesn't reuse past experience.

<strong>2. Training Instability</strong>: Actor-Critic can exhibit unstable training for several reasons:

<ul>
<li><strong>Bootstrapping with incorrect estimates</strong>: The critic learns from its own predictions ($V_\phi(s_{t+1})$ in the TD error). If the critic is initially wrong about state values, it can teach itself incorrect values, creating a vicious cycle.</li>
</ul>

  <strong>Mario Example</strong>: If the critic wrongly believes "standing before a pit" has value +200, it will compute small TD errors even when Mario falls. The critic won't learn the state is actually dangerous until much later.

<ul>
<li><strong>Moving targets problem</strong>: Both actor and critic update simultaneously:</li>
<p>
  - Actor's policy changes ‚Üí different states are visited ‚Üí critic's learning target shifts
  - Critic's values change ‚Üí actor's gradient direction changes
  - This creates a "chasing a moving target" scenario where neither network can fully converge
</p>
</ul>

<ul>
<li><strong>Correlated sequential data</strong>: On-policy data comes from consecutive game frames, which are highly similar. This correlation can amplify errors and slow convergence.</li>
</ul>

<strong>Why This Matters:</strong>
<ul>
<li>Training curves can spike up and down unpredictably</li>
<li>Hyperparameters (learning rates, network sizes) require careful tuning</li>
<li>Small changes can cause training to diverge</li>
</ul>

<blockquote>ü§î <strong>Next Question:</strong> Can we collect experience faster and more efficiently?</blockquote>

<hr>

<h3>4Ô∏è‚É£ A2C/A3C: <em>What if many Marios learn simultaneously?</em></h3>

<strong>The Insight:</strong>
<p>
Run multiple Mario agents in parallel environments. Each contributes to gradient estimates.
</p>

<strong>How it Helps:</strong>
<ul>
<li><strong>Faster data collection</strong>: More experience per second</li>
<li><strong>Decorrelated samples</strong>: Different Marios encounter different situations</li>
<li><strong>Smoother gradients</strong>: Averaging across agents reduces variance</li>
</ul>

<strong>‚úÖ The Improvement:</strong>
<p>
Training is faster and more stable.
</p>

<strong>‚ùå New Problem:</strong>
<p>
Even with these improvements, the policy can still make <strong>sudden, catastrophic changes</strong>.
</p>

<blockquote>ü§î <strong>Next Question:</strong> How do we prevent Mario from "forgetting" good strategies overnight?</blockquote>

<hr>

<h3>5Ô∏è‚É£ TRPO: <em>How do we ensure safe, gradual improvement?</em></h3>

<strong>The Problem:</strong>
<p>
A large policy update can make performance collapse. Imagine Mario suddenly changing from "jump when you see an enemy" to "never jump."
</p>

<strong>The Solution:</strong>
<p>
Add a <strong>trust region constraint</strong> ‚Äî restrict how much the new policy $\pi_\theta$ can differ from the old policy $\pi_{\theta_{\text{old}}}$:
</p>

<p>
$$
\begin{aligned}
\max_\theta \quad & \mathbb{E}_t\left[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} A_t\right] \\
\text{subject to} \quad & \mathbb{E}_t[\text{KL}(\pi_{\theta_{\text{old}}} \| \pi_\theta)] \le \delta
\end{aligned} \tag{4}
$$
</p>

<strong>Breaking Down Equation (4):</strong>

<strong>First, Let's Define the Two Policies:</strong>

<ul>
<li><strong>$\pi_{\theta_{\text{old}}}$ (Old Policy / Behavior Policy)</strong>:</li>
<p>
  - The policy that was used to <strong>collect the data</strong> (states, actions, rewards)
  - Has fixed parameters $\theta_{\text{old}}$ that do <strong>not</strong> change during the current update
  - We ran this policy in the environment to gather experience
  - Think of it as "the policy Mario was using when he played the game"
</p>
</ul>

<ul>
<li><strong>$\pi_\theta$ (New Policy / Target Policy)</strong>:</li>
<p>
  - The policy we are <strong>currently optimizing</strong> and want to improve
  - Has parameters $\theta$ that we are <strong>updating</strong> via gradient descent
  - We want to make this policy better based on the data collected by $\pi_{\theta_{\text{old}}}$
  - Think of it as "the improved policy Mario is learning"
</p>
</ul>

<strong>How They're Used in the Training Process:</strong>

<p>
1. <strong>Collect Data</strong>: Use $\pi_{\theta_{\text{old}}}$ to interact with environment
   - Mario plays the game using his current policy
   - We record: states $s_t$, actions $a_t$, rewards $r_t$, advantages $A_t$
</p>

<p>
2. <strong>Optimize New Policy</strong>: Update $\pi_\theta$ to maximize the objective, subject to the KL constraint
   - We improve Mario's policy based on what happened during gameplay
   - The probability ratio $\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ allows us to use old data to evaluate the new policy
</p>

<p>
3. <strong>Update and Repeat</strong>: After optimization, set $\theta_{\text{old}} \leftarrow \theta$
   - The new policy becomes the old policy for the next iteration
   - Mario's improved policy becomes his "current" policy
   - Collect new data with this updated policy and repeat
</p>

<strong>Why We Need Two Policies:</strong>

<ul>
<li><strong>Data reuse</strong>: TRPO collects data with $\pi_{\theta_{\text{old}}}$, but we want to improve $\pi_\theta$ using that data</li>
<li><strong>Importance sampling</strong>: The probability ratio corrects for the mismatch between the distribution of actions in our data (from $\pi_{\theta_{\text{old}}}$) and the distribution we're evaluating (from $\pi_\theta$)</li>
<li><strong>Stability</strong>: The KL constraint ensures $\pi_\theta$ doesn't deviate too far from $\pi_{\theta_{\text{old}}}$, keeping the importance sampling valid</li>
</ul>

<strong>Part 1: The Objective (What We Want to Maximize)</strong>

<p>
$$\mathbb{E}_t\left[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} A_t\right]$$
</p>

<p>
This is called the <strong>surrogate objective</strong> with <strong>importance sampling</strong>:
</p>

<ul>
<li><strong>Probability ratio</strong> $\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$:</li>
<p>
  - Measures how much more (or less) likely action $a_t$ is under new policy $\pi_\theta$ compared to old policy $\pi_{\theta_{\text{old}}}$
  - If ratio > 1: New policy prefers this action more than old policy
  - If ratio < 1: New policy prefers this action less than old policy
  - If ratio = 1: Both policies assign same probability
</p>
</ul>

<ul>
<li><strong>Advantage</strong> $A_t$:</li>
<p>
  - Positive advantage ($A_t > 0$): Action was better than expected ‚Üí increase its probability (ratio > 1)
  - Negative advantage ($A_t < 0$): Action was worse than expected ‚Üí decrease its probability (ratio < 1)
</p>
</ul>

<ul>
<li><strong>Why importance sampling?</strong>: We collected data using $\pi_{\theta_{\text{old}}}$, but we want to evaluate $\pi_\theta$. The ratio corrects for this distribution mismatch, allowing us to reuse old data.</li>
</ul>

<strong>Part 2: The Constraint (What We Must Satisfy)</strong>

<p>
$$\mathbb{E}_t[\text{KL}(\pi_{\theta_{\text{old}}} \| \pi_\theta)] \le \delta$$
</p>

<ul>
<li><strong>KL divergence</strong> $\text{KL}(\pi_{\theta_{\text{old}}} \| \pi_\theta)$:</li>
<p>
  - Measures how different the new policy distribution is from the old policy distribution
  - Specifically: $\text{KL}(P \| Q) = \sum_a P(a) \log \frac{P(a)}{Q(a)}$ (for discrete actions)
  - Always non-negative: $\text{KL} \geq 0$, with $\text{KL} = 0$ only when policies are identical
  - Not symmetric: $\text{KL}(P \| Q) \neq \text{KL}(Q \| P)$ in general
</p>
</ul>

<ul>
<li><strong>Trust region size</strong> $\delta$:</li>
<p>
  - A small constant (e.g., $\delta = 0.01$)
  - Limits how much the policy can change in one update
  - Smaller $\delta$ ‚Üí more conservative updates ‚Üí more stable but slower learning
</p>
</ul>

<strong>Why This Formulation Works:</strong>

<p>
1. <strong>Monotonic Improvement Guarantee</strong>: TRPO provides theoretical guarantees that performance won't decrease (under certain assumptions), because we only update the policy within a safe region
</p>

<p>
2. <strong>Prevents Catastrophic Forgetting</strong>: The KL constraint ensures the new policy doesn't deviate too far from the old policy, preventing sudden collapse in performance
</p>

<p>
3. <strong>Maintains On-Policy Assumption</strong>: By keeping policies similar, data collected from $\pi_{\theta_{\text{old}}}$ remains approximately valid for $\pi_\theta$
</p>

<strong>Mario Example:</strong>

<p>
Suppose at state "goomba ahead":
</p>
<ul>
<li>Old policy: $\pi_{\theta_{\text{old}}}(\text{jump}|s) = 0.6$, $\pi_{\theta_{\text{old}}}(\text{right}|s) = 0.3$</li>
<li>Advantage: $A(\text{jump}) = +50$ (jumping worked well!)</li>
</ul>

<p>
Without constraint, we might update to:
</p>
<ul>
<li>New policy: $\pi_{\theta}(\text{jump}|s) = 0.99$ (too aggressive!)</li>
</ul>

<p>
With KL constraint ($\delta = 0.01$), TRPO limits the update:
</p>
<ul>
<li>New policy: $\pi_{\theta}(\text{jump}|s) = 0.65$ (safer, gradual increase)</li>
</ul>

<p>
The constraint says: "Mario, you can increase the probability of jumping, but not by too much in one update. Let's be conservative."
</p>

<strong>Mario's Experience:</strong>
<p>
Mario takes small, safe steps in learning. He doesn't radically change his jumping strategy overnight.
</p>

<strong>‚úÖ The Improvement:</strong>
<p>
Much more stable training. Performance rarely degrades.
</p>

<strong>‚ùå New Problem:</strong>
<p>
The KL constraint requires <strong>second-order optimization</strong> (computing Hessians), which is computationally expensive.
</p>

<strong>Why does TRPO need second-order optimization?</strong>

<p>
The KL constraint $\mathbb{E}_t[\text{KL}(\pi_{\theta_{\text{old}}} \| \pi_\theta)] \le \delta$ is a <strong>hard constraint</strong> that must be satisfied. To solve this constrained optimization problem efficiently, TRPO uses:
</p>

<p>
1. <strong>Second-order Taylor approximation</strong>: Approximate the KL divergence around the current parameters using both first derivatives (gradient) and second derivatives (Hessian/curvature)
</p>

<p>
2. <strong>Natural gradient descent</strong>: The optimal update direction requires computing:
   $$\theta_{\text{new}} = \theta_{\text{old}} + \alpha \mathbf{F}^{-1} \nabla_\theta J(\theta)$$
   where $\mathbf{F}$ is the <strong>Fisher information matrix</strong> (the Hessian of the KL divergence)
</p>

<p>
3. <strong>Conjugate gradient method</strong>: To avoid explicitly forming the full Hessian matrix, TRPO uses conjugate gradient, which requires Hessian-vector products
</p>

<strong>Why is this expensive?</strong>

<ul>
<li><strong>Memory</strong>: For a neural network with $n$ parameters, the Hessian is an $n \times n$ matrix. For millions of parameters, this is impractical to store</li>
<li><strong>Computation</strong>: Even though TRPO uses Hessian-vector products (avoiding explicit matrix formation), this still requires multiple backward passes through the network</li>
<li><strong>Complexity</strong>: Each policy update requires solving a constrained optimization problem, much slower than simple gradient descent</li>
</ul>

<strong>Concrete comparison:</strong>
<ul>
<li><strong>First-order methods</strong> (like standard gradient descent): Compute gradient ‚Üí take step. Fast and simple.</li>
<li><strong>TRPO (second-order)</strong>: Compute gradient ‚Üí compute Hessian-vector products ‚Üí solve constrained optimization ‚Üí take step. Slow but stable.</li>
</ul>

<p>
This is why researchers wanted to find a simpler alternative that keeps TRPO's stability...
</p>

<blockquote>ü§î <strong>Next Question:</strong> Can we keep the stability of TRPO but make it simpler?</blockquote>

<hr>

<h3>6Ô∏è‚É£ PPO: <em>Can we simplify safe updates?</em></h3>

<strong>The Breakthrough:</strong>
<p>
Replace the hard KL constraint with a simple <strong>clipped objective</strong> that achieves the same goal.
</p>

<strong>The Clipped Surrogate Loss:</strong>

<p>
$$
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) A_t, \, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right] \tag{5}
$$
</p>

<p>
where the <strong>probability ratio</strong> is:
</p>

<p>
$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} \tag{6}
$$
</p>

<strong>What the Clip Does:</strong>
<ul>
<li>If $r_t(\theta)$ moves outside $[1-\epsilon, 1+\epsilon]$ (typically $\epsilon=0.2$), we stop the gradient</li>
<li>This prevents large policy changes without expensive second-order computations</li>
</ul>

<strong>Mario's Experience:</strong>
<p>
If Mario's new policy tries to change too much (say, more than ¬±20%), we gently stop it, keeping updates stable.
</p>

<strong>‚úÖ Why PPO Won:</strong>
<ul>
<li>Simple to implement (just a few lines of code)</li>
<li>Computationally efficient (first-order optimization)</li>
<li>Stable and reliable across diverse tasks</li>
<li>State-of-the-art performance</li>
</ul>

<strong>This is the first algorithm implemented in the main repository.</strong>

<strong>‚ùå The Remaining Challenge:</strong>
<p>
PPO learns only from <strong>real interactions</strong> with the environment. Mario must play millions of frames to learn. This is slow, expensive, and sample-inefficient.
</p>

<blockquote>ü§î <strong>Further Question:</strong> What if Mario could <em>imagine</em> playing the game instead of always playing for real?</blockquote>

<hr>

<h3>7Ô∏è‚É£ DreamerV3: <em>Can we learn by dreaming?</em></h3>

<strong>The Paradigm Shift:</strong>
<p>
Instead of learning only from real experience, teach Mario to:
1. Build an <strong>internal model</strong> of how the game works
2. <strong>Imagine</strong> future scenarios inside this model
3. Practice and improve his policy inside his imagination
4. Periodically collect small batches of real experience to keep the model accurate
</p>

<p>
This is called <strong>model-based reinforcement learning</strong>.
</p>

<p>
For example, for every 100 real frames Mario plays:
</p>
<ul>
<li>PPO trains on those 100 frames, then discards them</li>
<li>DreamerV3 trains on those 100 frames, PLUS 10,000+ imagined frames generated by the world model</li>
</ul>

<p>
The world model is updated <strong>regularly</strong> with real data to stay accurate, but most learning happens in imagination. This achieves 10-100x better sample efficiency!
</p>

<hr>

<h4>üß† How DreamerV3 Works</h4>

<strong>Three Core Components:</strong>

<strong>1. World Model (RSSM - Recurrent State Space Model)</strong>

<p>
The world model learns to simulate the environment using a <strong>two-part latent state</strong>:
</p>
<ul>
<li><strong>$h_t$</strong>: Deterministic recurrent state (captures deterministic long-term dependencies)</li>
<li><strong>$z_t$</strong>: Stochastic latent state (captures stochastic variations)</li>
</ul>

<p>
The complete latent representation is the pair $(h_t, z_t)$.
</p>

<strong>Core Components:</strong>

<ul>
<li><strong>Recurrent Model</strong>: $h_{t+1} = f_\phi(h_t, z_t, a_t)$</li>
<p>
  - Deterministic GRU that updates the recurrent state
  - Inputs: previous recurrent state $h_t$, stochastic state $z_t$, action $a_t$
  - Output: next recurrent state $h_{t+1}$
</p>
</ul>

<ul>
<li><strong>Representation Model (Posterior)</strong>: $q_\phi(z_t | h_t, o_t)$</li>
<p>
  - Used during <strong>training</strong> with real observations
  - Infers stochastic state from recurrent state AND observation
  - Encodes observation into the latent dynamics
</p>
</ul>

<ul>
<li><strong>Transition Model (Prior)</strong>: $p_\phi(z_t | h_t)$</li>
<p>
  - Used during <strong>imagination</strong> (no observations available)
  - Predicts stochastic state from only the recurrent state
  - Enables imagining future states without real observations
</p>
</ul>

<ul>
<li><strong>Decoder</strong>: $p_\phi(o_t | h_t, z_t)$</li>
<p>
  - Reconstructs observation from the full latent state
  - Trained to minimize reconstruction loss
</p>
</ul>

<ul>
<li><strong>Reward Predictor</strong>: $p_\phi(r_t | h_t, z_t)$</li>
<p>
  - Predicts reward from the latent state
  - Enables reward estimation during imagination
</p>
</ul>

<ul>
<li><strong>Continue Predictor</strong>: $p_\phi(c_t | h_t, z_t)$</li>
<p>
  - Predicts if episode continues (discount factor)
  - Handles episode termination in imagination
</p>
</ul>

<strong>2. Imagination Rollouts</strong>

<p>
Starting from a real latent state $(h_t, z_t)$, use the world model to generate <strong>imaginary trajectories</strong>:
</p>

<p>
$$
(h_t, z_t) \xrightarrow{a_t} (h_{t+1}, z_{t+1}) \xrightarrow{a_{t+1}} (h_{t+2}, z_{t+2}) \xrightarrow{a_{t+2}} \cdots
$$
</p>

<strong>How imagination works:</strong>
<p>
1. Sample action $a_t$ from policy $\pi_\theta(a|h_t, z_t)$
2. Update recurrent state: $h_{t+1} = f_\phi(h_t, z_t, a_t)$
3. Sample stochastic state from <strong>prior</strong>: $z_{t+1} \sim p_\phi(\cdot | h_{t+1})$ (no observation needed!)
4. Predict reward: $\hat{r}_t = p_\phi(r_t | h_t, z_t)$
5. Predict continuation: $\hat{c}_t = p_\phi(c_t | h_t, z_t)$
6. Repeat for many timesteps
</p>

<p>
These trajectories are generated entirely inside the model ‚Äî no real environment interaction needed.
</p>

<strong>3. Actor-Critic in Latent Space</strong>
<p>
Train the policy $\pi_\theta(a|h, z)$ and value function $V_\psi(h, z)$ on imagined trajectories:
</p>

<ul>
<li><strong>Actor objective</strong>: Maximize imagined returns</li>
<li><strong>Critic objective</strong>: Accurately predict imagined values</li>
</ul>

<hr>

<h4>üîÑ Training Loop</h4>

<p>
``<code>
1. Collect a batch of real experience from the environment
2. Train the world model to predict observations, rewards, and continuations
3. Sample real states, then generate imaginary rollouts using the world model
4. Train actor and critic on imaginary experience
5. Repeat
</code>``
</p>

<strong>Mario's Experience:</strong>
<p>
Mario plays the game for a short time, watches what happens, and learns how the world behaves (enemies move, blocks break, jumping has consequences). Then, he <strong>mentally simulates</strong> thousands of different scenarios:
</p>
<ul>
<li>"What if I jump here?"</li>
<li>"What if I run past this enemy?"</li>
<li>"What if I take the pipe?"</li>
</ul>

<p>
He tests all these strategies in his mind before trying them in the real game.
</p>

<hr>

<h4>üéØ Why DreamerV3 is Powerful</h4>

<strong>Benefits:</strong>
<ul>
<li><strong>Sample Efficiency</strong>: Learns from far fewer real environment steps</li>
<li><strong>Planning</strong>: Can imagine and evaluate plans before executing them</li>
<li><strong>Generalization</strong>: World model captures environment dynamics, enabling better transfer</li>
<li><strong>Safety</strong>: Most learning happens in imagination, not in risky real environments</li>
</ul>

<strong>Trade-offs:</strong>
<ul>
<li><strong>Complexity</strong>: More components to implement and tune</li>
<li><strong>Model Error</strong>: If the world model is wrong, the policy may learn suboptimal behaviors</li>
<li><strong>Computational Cost</strong>: Training the world model adds overhead</li>
</ul>

<strong>This is the second algorithm implemented in the main repository.</strong>

<hr>

<h2>üîÅ Summary: The Journey from REINFORCE to DreamerV3</h2>

<p>
| Algorithm | Key Question | Key Innovation | Mario's Learning Style |
|-----------|--------------|----------------|------------------------|
| <strong>REINFORCE</strong> | Can we learn from rewards? | Policy gradient from episode returns | Pure trial and error |
| <strong>+ Baseline</strong> | How do we reduce noise? | Subtract expected value | Learns relative success |
| <strong>Actor-Critic</strong> | Can we learn online? | TD error for immediate feedback | Real-time learning |
| <strong>A2C/A3C</strong> | Can we learn faster? | Parallel environments | Multiple Marios learn together |
| <strong>TRPO</strong> | How do we prevent collapse? | Trust region constraint | Safe, gradual improvement |
| <strong>PPO</strong> | Can we simplify safety? | Clipped surrogate objective | Efficient and stable |
| <strong>DreamerV3</strong> | Can we learn by imagining? | World model + imagination rollouts | Dreams before acting |
</p>

<hr>

<h2>üìò References and Further Reading</h2>

<h3>Original Papers</h3>

<p>
1. <strong>REINFORCE</strong>
   Williams, R. J. (1992). <em>Simple statistical gradient-following algorithms for connectionist reinforcement learning.</em> Machine Learning, 8, 229-256.
</p>

<p>
2. <strong>Actor-Critic Methods</strong>
   Sutton, R. S., & Barto, A. G. (2018). <em>Reinforcement Learning: An Introduction</em> (2nd ed.). MIT Press.
</p>

<p>
3. <strong>A3C</strong>
   Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., & Kavukcuoglu, K. (2016). <em>Asynchronous Methods for Deep Reinforcement Learning.</em> ICML.
</p>

<p>
4. <strong>TRPO</strong>
   Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). <em>Trust Region Policy Optimization.</em> ICML.
</p>

<p>
5. <strong>PPO</strong>
   Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). <em>Proximal Policy Optimization Algorithms.</em> arXiv:1707.06347.
</p>

<p>
6. <strong>GAE (Generalized Advantage Estimation)</strong>
   Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2015). <em>High-Dimensional Continuous Control Using Generalized Advantage Estimation.</em> arXiv:1506.02438.
</p>

<p>
7. <strong>DreamerV1</strong>
   Hafner, D., Lillicrap, T., Ba, J., & Norouzi, M. (2019). <em>Dream to Control: Learning Behaviors by Latent Imagination.</em> ICLR 2020.
</p>

<p>
8. <strong>DreamerV2</strong>
   Hafner, D., Lillicrap, T., Norouzi, M., & Ba, J. (2020). <em>Mastering Atari with Discrete World Models.</em> ICLR 2021.
</p>

<p>
9. <strong>DreamerV3</strong>
   Hafner, D., Pasukonis, J., Ba, J., & Lillicrap, T. (2023). <em>Mastering Diverse Domains through World Models.</em> arXiv:2301.04104.
</p>

<h3>Helpful Resources</h3>

<ul>
<li><strong><a href="https://spinningup.openai.com/">Spinning Up in Deep RL (OpenAI)</a></strong></li>
<p>
  Excellent educational resource with clear explanations and code examples
</p>
</ul>

<ul>
<li><strong><a href="https://huggingface.co/deep-rl-course">Deep RL Course (Hugging Face)</a></strong></li>
<p>
  Free course covering the fundamentals to advanced topics
</p>
</ul>

<ul>
<li><strong><a href="https://github.com/danijar/dreamerv3">DreamerV3 Official Implementation (JAX)</a></strong></li>
<p>
  Official implementation by Danijar Hafner
</p>
</ul>

<ul>
<li><strong><a href="http://incompleteideas.net/book/the-book-2nd.html">Sutton & Barto RL Book (Free Online)</a></strong></li>
<p>
  The canonical textbook on reinforcement learning
</p>
</ul>

<ul>
<li><strong><a href="https://www.youtube.com/watch?v=5P7I-xPq8u8">PPO Explained (Video by Arxiv Insights)</a></strong></li>
<p>
  Visual explanation of PPO
</p>
</ul>

<ul>
<li><strong><a href="https://worldmodels.github.io/">World Models Paper (Ha & Schmidhuber, 2018)</a></strong></li>
<p>
  Precursor to Dreamer with excellent visualizations
</p>
</ul>

<ul>
<li><strong><a href="https://github.com/openai/baselines">OpenAI Baselines</a></strong></li>
<p>
  High-quality implementations of RL algorithms
</p>
</ul>

<hr>

<strong><a href="README.md">‚Üê Back to Main README</a></strong>


</body>
</html>